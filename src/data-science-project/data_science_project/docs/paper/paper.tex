%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{float} 
\usepackage{graphicx}
\usepackage{url}

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Grupo Bimbo Inventory Demand Data Science Report}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Juan Serrano$^{1}$, Nicolas Guevara$^{2}$ and Giovanny Moreno$^{3}$% <-this % stops a space
\thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$We belong to the Faculty of Systems Engineering, Francisco Jose de Caldas University, Bogota, Colombia
        {\tt\small https://www.udistrital.edu.co/inicio}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
This study focuses on forecasting the demand for Grupo Bimbo's products using machine learning techniques. Grupo Bimbo, a large Mexican bakery company, provided a dataset encompassing weekly sales and returns transactions. Additionally, bi-weekly inflation and consumer confidence index data were collected via web scraping to enhance the model's predictive power. The primary objective is to predict the adjusted demand for products to optimize inventory management. We employed two machine learning models: XGBoost and Random Forest. The data underwent extensive preprocessing, including handling missing values, feature engineering, and encoding categorical variables. Model performance was evaluated based on root mean squared error (RMSE). Our results indicate that XGBoost outperformed Random Forest, providing a more accurate prediction of product demand. These insights can help Grupo Bimbo enhance its inventory management and reduce operational costs.       This study focuses on forecasting the demand for Grupo Bimbo's products using machine learning techniques. Grupo Bimbo, a large Mexican bakery company, provided a dataset encompassing weekly sales and returns transactions. The primary objective is to predict the adjusted demand for products to optimize inventory management. We employed two machine learning models: XGBoost and Random Forest. The data underwent extensive preprocessing, including handling missing values, feature engineering, and encoding categorical variables. Model performance was evaluated based on root mean squared error (RMSE). Our results indicate that XGBoost outperformed Random Forest, providing a more accurate prediction of product demand. These insights can help Grupo Bimbo enhance its inventory management and reduce operational costs.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Effective inventory management is crucial for companies in the food industry, where perishable goods and fluctuating demand present significant challenges. Grupo Bimbo, one of the largest bakery companies globally, faces the task of accurately predicting product demand to minimize waste and ensure optimal stock levels. Traditional inventory management approaches often fall short in handling the complexity and scale of data involved. This paper explores the application of machine learning models to predict product demand, using a dataset provided by Grupo Bimbo. Additionally, we incorporated external economic indicators, such as bi-weekly inflation rates and the consumer confidence index, collected through web scraping. We focus on two popular models, XGBoost and Random Forest, to determine their efficacy in forecasting adjusted demand. The goal is to provide a robust predictive framework that can aid in better decision-making and resource allocation.
\section{METHOD AND MATERIALS}

\subsection{Dataset Description}
The dataset used in this study comprises approximately 7 million records from Grupo Bimbo, capturing weekly sales transactions and returns. The dataset includes the following features:
\begin{itemize}
        \item \textbf{Semana (Week)}: The week number (from Thursday to Wednesday).
        \item \textbf{Agencia\_ID (Sales Depot ID)}: Unique identifier for sales depots.
        \item \textbf{Canal\_ID (Sales Channel ID)}: Identifier for the sales channel.
        \item \textbf{Ruta\_SAK (Route ID)}: Delivery route identifier within a sales depot.
        \item \textbf{Cliente\_ID (Client ID)}: Unique identifier for each client.
        \item \textbf{NombreCliente (Client Name)}: Descriptive name of the client.
        \item \textbf{Producto\_ID (Product ID)}: Unique identifier for each product.
        \item \textbf{NombreProducto (Product Name)}: Descriptive name of the product.
        \item \textbf{Venta\_uni\_hoy (Sales Units This Week)}: Number of units sold this week.
        \item \textbf{Venta\_hoy (Sales This Week in Pesos)}: Total sales in monetary units this week.
        \item \textbf{Dev\_uni\_proxima (Returns Units Next Week)}: Number of units returned next week.
        \item \textbf{Dev\_proxima (Returns Next Week in Pesos)}: Total monetary value of returns next week.
        \item \textbf{Demanda\_uni\_equil (Adjusted Demand)}: Target variable representing the adjusted demand.
\end{itemize}

\subsection{Additional Features}
To enhance the predictive accuracy, we collected external economic indicators through web scraping:

\begin{itemize}
        \item \textbf{Bi-weekly Inflation Rate}: Inflation data from March 31 to June 1, 2016.
        \item \textbf{Consumer Confidence Index}: Consumer confidence index data for the same period.
\end{itemize}

\subsection{Data Preprocessing}
The preprocessing steps undertaken in this study involved several key processes to ensure the data was clean, consistent, and suitable for model training. Below are the detailed steps followed:
\begin{itemize}                
        \item \textbf{Data Cleaning}: 
        \begin{itemize}
                \item Handling Missing Values: Missing values in the dataset were identified and appropriately handled. For numerical columns, missing values were filled with the median of the column, while for categorical columns, the mode was used.        
        \end{itemize}
        \item Outliers Removal: Outliers were detected using z-score and IQR methods and were removed or capped to prevent them from skewing the model results.
        
        \item Data Transformation:
        \begin{itemize}
                \item Normalization and Scaling: Numerical features were normalized and scaled to ensure all features contribute equally to the model. StandardScaler from sklearn was used to transform features to have zero mean and unit variance.
        \end{itemize}
        
        \item \textbf{Feature Engineering}: Creating new features such as lagged variables to capture temporal dependencies and integrating external economic indicators.
        \begin{itemize}
                \item Temporal Features: Change to week features to datetime format and put as index.
                \item Interaction Features: Interaction features between different variables were created to capture complex relationships within the data.
                \begin{itemize}
                        \item \textbf{Sales\_Growth\_Rate\_1}: Demand for the same product in the previous week.
                        \item \textbf{Sales\_Growth\_Rate\_2}: Demand for the same product two weeks ago.   
                \end{itemize}
                \item External Economic Indicators: The bi-weekly inflation rate and consumer confidence index were merged with the primary dataset to capture the economic conditions impacting product demand.
        \end{itemize}
\end{itemize}

\begin{figure}[H] 
        \begin{center}
        \centering
        \includegraphics[width=0.5\textwidth]{images/output.png}
        \caption{Heatmap Feature Information}
        \end{center}
\end{figure} 

\subsection{Modeling}
Two machine learning models were chosen for this study based on their ability to handle large datasets and complex relationships: XGBoost and Random Forest.
\begin{itemize}
        \item XGBoost: XGBoost (Extreme Gradient Boosting) is an efficient and scalable implementation of gradient boosting framework by Friedman et al. It provides parallel tree boosting that solves many data science problems in a fast and accurate way.
        \begin{itemize}
                \item $\textbf{Parameters}$: $n\_estimators=200$, $max\_depth=8$, $learning\_rate=0.2$, $subsample=0.9$, $colsample\_bytree=0.8$, $n\_jobs=-1$, and $random\_state=42$.
        \end{itemize}
        \item Random Forest is an ensemble learning method for classification, regression, and other tasks that operates by constructing a multitude of decision trees at training time.
        \begin{itemize}
                \item $\textbf{Parameters}$: $n\_estimators=110$, $min\_samples\_split=10$, $min\_samples\_leaf=5$, $n\_jobs=-1$, and $random\_state=42$.
        \end{itemize}
\end{itemize}

\subsection{Evaluation}
Model performance was evaluated using the root mean squared error (RMSE) as the primary metric. RMSE is a standard way to measure the error of a model in predicting quantitative data. Additionally, other metrics such as R-squared, Mean Absolute Error (MAE), and Mean Squared Error (MSE) were also considered for a comprehensive evaluation.
\begin{itemize}
        \item XGBoost Performance: 
        
        \begin{itemize}
                \item RMSE: The RMSE for the XGBoost model was X.
                \item Other Metrics:
                \begin{itemize}
                        \item R-squared: The model's R-squared value was Y.
                        \item MAE: The Mean Absolute Error for the model was Z.
                        \item MSE: The Mean Squared Error for the model was W.
                \end{itemize}
        \end{itemize}
        
        \item Random Forest Performance: is an ensemble learning method for classification, regression, and other tasks that operates by constructing a multitude of decision trees at training time.
        
        \begin{itemize}
                \item RMSE: The RMSE for the Random Forest model was A.
                \item Other Metrics:
                \begin{itemize}
                        \item R-squared: The model's R-squared value was B.
                        \item MAE: The Mean Absolute Error for the model was C.
                        \item MSE: The Mean Squared Error for the model was D.
                \end{itemize}
        \end{itemize}
\end{itemize}

\section{RESULTS}
The performance of the XGBoost and Random Forest models were evaluated based on the RMSE. The Random Forest model achieved an RMSE of X, indicating better predictive accuracy compared to the XGBoost model, which had an RMSE of Y. The superior performance of Random Forest can be attributed to its ability to handle high-dimensional data and its robustness to overfitting. The inclusion of external economic indicators, such as bi-weekly inflation rates and the consumer confidence index, further enhanced the model's predictive capabilities.

\begin{figure}[H] 
        \begin{center}
        \centering
        \includegraphics[width=0.5\textwidth]{images/comparison.jpg}
        \caption{Actual vs Predict Adjusted Demand RandomForest}
        \end{center}
\end{figure} 

\begin{figure}[H] 
        \begin{center}
        \centering
        \includegraphics[width=0.5\textwidth]{images/comparisonxg.jpg}
        \caption{Actual vs Predict Adjusted Demand XGBoost}
        \end{center}
\end{figure} 


\section{CONCLUSIONS}

This study demonstrates the potential of machine learning models in forecasting product demand for Grupo Bimbo. The Random Forest model outperformed the XGBoost model, offering more accurate predictions of adjusted demand. The integration of external economic indicators, such as bi-weekly inflation rates and the consumer confidence index, proved beneficial in enhancing the model's accuracy. These insights can be leveraged to improve inventory management, reduce waste, and optimize resource allocation. Future work could explore the integration of additional features, such as macroeconomic indicators and weather data, to further enhance prediction accuracy. Additionally, implementing these models in a real-time production environment could provide continuous insights and adaptive inventory strategies.

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}

\bibitem{c1} Kaggle, "Grupo Bimbo Inventory Demand," Kaggle, 2016. [Online]. Available: \url{https://www.kaggle.com/competitions/grupo-bimbo-inventory-demand/overview}. [Accessed: 15-Jul-2024].

\bibitem{c2} INEGI, "API del Banco de Indicadores," INEGI, 2024. [Online]. Available: \url{https://www.inegi.org.mx/servicios/api_indicadores.html#idMetodoIndicadoresInegi}. [Accessed: 15-Jul-2024].

\end{thebibliography}

\end{document}
